"""
Modular Configuration System for Captcha Architecture.

Hierarchy:
    ExperimentConfig
    ├── DatasetConfig
    ├── ModelConfig
    │   ├── EncoderConfig (type-specific)
    │   ├── ProjectorConfig (type-specific)
    │   ├── SequenceModelConfig (type-specific)
    │   └── HeadConfig (type-specific)
    └── TrainingConfig

All hyperparameters are first-class citizens with no hardcoded values in components.
"""
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Union, Callable
from enum import Enum
import torch
from transformer_lens import HookedTransformerConfig


# ============================================================================
# ENUMS FOR TYPE SAFETY
# ============================================================================

class ActivationType(str, Enum):
    GELU = "gelu"
    RELU = "relu"
    SILU = "silu"
    TANH = "tanh"


class NormalizationType(str, Enum):
    RMS = "RMS"
    LAYER = "layer"
    BATCH = "batch"
    RMSNormPre = "RMSNormPre"
    LayerNorm = "LayerNorm"


class PositionalEmbeddingType(str, Enum):
    ROTARY = "rotary"
    STANDARD = "standard"
    NONE = "none"


class AttentionDirection(str, Enum):
    CAUSAL = "causal"
    BIDIRECTIONAL = "bidirectional"


class PoolingType(str, Enum):
    MEAN = "mean"
    MAX = "max"
    FIRST = "first"


class LossType(str, Enum):
    CTC = "ctc"
    CROSS_ENTROPY = "cross_entropy"
    FOCAL = "focal"


class OptimizerType(str, Enum):
    ADAM = "adam"
    ADAMW = "adamw"
    SGD = "sgd"


class TaskType(str, Enum):
    GENERATION = "generation"
    CLASSIFICATION = "classification"


class PipelineType(str, Enum):
    STANDARD_GENERATION = "standard_generation" # Encoder -> Adapter -> Seq -> Head
    STANDARD_CLASSIFICATION = "standard_classification" # Encoder -> Adapter -> Head
    SEQUENCE_CLASSIFICATION = "sequence_classification" # Encoder -> Adapter -> Seq -> Pool -> Head


# ============================================================================
# DATASET CONFIG
# ============================================================================

@dataclass
class DatasetConfig:
    """Configuration for dataset generation and processing."""
    
    # Image dimensions
    width: int = 200
    height: int = 80
    
    # Font configuration
    fonts: Optional[List[str]] = None
    font_root: Optional[str] = None
    train_font_root: Optional[str] = None
    val_font_root: Optional[str] = None
    font_sizes: Optional[List[int]] = None
    max_fonts_per_family: int = 2
    
    # Content configuration
    word_path: Optional[str] = None
    fixed_words: Optional[List[str]] = None
    
    # Noise configuration
    noise_bg_density: int = 5000
    add_noise_dots: bool = True
    add_noise_curve: bool = True
    
    # Spacing configuration
    extra_spacing: int = -5
    spacing_jitter: int = 6
    
    # Color configuration
    bg_color: Optional[tuple[int, int, int]] = None
    fg_color: Optional[tuple[int, int, int, int]] = None
    
    # Output configuration
    image_ext: str = "png"
    
    # Text transformation
    word_transform: Optional[str] = None  # e.g., "random_capitalize"
    random_capitalize: bool = True
    
    # Fine-grained Captcha Distortion (Captcha Library Defaults overrides)
    character_offset_dx: Optional[tuple[int, int]] = (0, 4)
    character_offset_dy: Optional[tuple[int, int]] = (0, 6)
    character_rotate: Optional[tuple[int, int]] = (-30, 30)
    character_warp_dx: Optional[tuple[float, float]] = (0.1, 0.3)
    character_warp_dy: Optional[tuple[float, float]] = (0.2, 0.3)
    word_space_probability: float = 0.5
    word_offset_dx: float = 0.25
    
    # Processing configuration
    target_height: int = 80  # For resizing
    width_divisor: int = 4   # Width must be divisible by this
    width_bias: int = 0      # Bias for width formula (width = divisor * k + bias)
    resize_mode: str = "variable" # "variable" (aspect-ratio preserving) or "fixed" (stretch to width)
    
    # On-the-fly Generation Config
    vocab: str = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
    min_word_len: int = 4
    max_word_len: int = 8
    
    # Flip Set Configuration
    use_flip_set: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'width': self.width,
            'height': self.height,
            'noise_bg_density': self.noise_bg_density,
            'add_noise_dots': self.add_noise_dots,
            'add_noise_curve': self.add_noise_curve,
            'extra_spacing': self.extra_spacing,
            'spacing_jitter': self.spacing_jitter,
            'target_height': self.target_height,
            'width_divisor': self.width_divisor,
            'width_bias': self.width_bias,
            'resize_mode': self.resize_mode,
            'vocab': self.vocab,
            'min_word_len': self.min_word_len,
            'max_word_len': self.max_word_len,
        }


# ============================================================================
# ENCODER CONFIGS
# ============================================================================

@dataclass
class ConvNextEncoderConfig:
    """Configuration for ConvNextEncoder."""
    
    # Stage dimensions
    dims: List[int] = field(default_factory=lambda: [64, 128, 256, 512])
    
    # Stage block counts (per stage, in order)
    stage_block_counts: List[int] = field(default_factory=lambda: [2, 2, 6, 2])
    
    # Stem configuration
    stem_kernel_size: tuple[int, int] | int = 4
    stem_stride: tuple[int, int] | int = 4
    stem_padding: tuple[int, int] | int = 0
    stem_in_channels: int = 3
    
    # Downsample configuration
    # List of strides for the 3 downsample blocks (between stage 1-2, 2-3, 3-4)
    downsample_strides: List[tuple[int, int]] = field(default_factory=lambda: [(2, 2), (2, 2), (2, 2)])
    
    # List of kernels for the 3 downsample blocks
    # If not specified, defaults to matching the stride (handled in Encoder class)
    downsample_kernels: Optional[List[tuple[int, int]]] = None

    downsample_padding: Optional[List[tuple[int, int]]] = None
    
    # ConvNextBlock configuration
    convnext_kernel_size: int = 7
    convnext_drop_path_rate: float = 0.0
    convnext_layer_scale_init_value: float = 1e-6
    convnext_activation: ActivationType = ActivationType.GELU
    convnext_expansion_ratio: int = 4  # MLP expansion in ConvNextBlock
    
    # Normalization
    norm_eps: float = 1e-6
    
    def __post_init__(self):
        if len(self.stage_block_counts) != len(self.dims):
            raise ValueError(f"stage_block_counts length ({len(self.stage_block_counts)}) must match dims length ({len(self.dims)})")
        if len(self.downsample_strides) != 3:
             raise ValueError(f"downsample_strides length ({len(self.downsample_strides)}) must be 3")
        if self.downsample_kernels is not None and len(self.downsample_kernels) != 3:
             raise ValueError(f"downsample_kernels length ({len(self.downsample_kernels)}) must be 3")
        if self.downsample_padding is not None and len(self.downsample_padding) != 3:
             raise ValueError(f"downsample_padding length ({len(self.downsample_padding)}) must be 3")

@dataclass
class ResNetEncoderConfig:
    """Configuration for ResNetEncoder."""
    
    # Checkpoints or specific ResNet types could be added here
    # For now, we assume standard custom ResNet from src/architecture/components/encoders.py
    
    # Internal dimensions
    dims: List[int] = field(default_factory=lambda: [64, 128, 256, 512])
    stage_block_counts: List[int] = field(default_factory=lambda: [2, 2, 6, 2])
    
    stem_kernel_size: tuple[int, int] | int = 4
    stem_stride: tuple[int, int] | int = 4
    stem_padding: tuple[int, int] | int = 0
    stem_in_channels: int = 3
    
    # List of strides for the 3 downsample blocks
    downsample_strides: List[tuple[int, int]] = field(default_factory=lambda: [(2, 2), (2, 2), (2, 2)])
    
    # List of kernels
    downsample_kernels: Optional[List[tuple[int, int]]] = None

    downsample_padding: Optional[List[tuple[int, int]]] = None

    def __post_init__(self):
        if len(self.stage_block_counts) != len(self.dims):
            raise ValueError(f"stage_block_counts length ({len(self.stage_block_counts)}) must match dims length ({len(self.dims)})")
        if len(self.downsample_strides) != 3:
             raise ValueError(f"downsample_strides length ({len(self.downsample_strides)}) must be 3")
        if self.downsample_kernels is not None and len(self.downsample_kernels) != 3:
             raise ValueError(f"downsample_kernels length ({len(self.downsample_kernels)}) must be 3")
        if self.downsample_padding is not None and len(self.downsample_padding) != 3:
             raise ValueError(f"downsample_padding length ({len(self.downsample_padding)}) must be 3")

# Union type for encoder configs
EncoderConfig = Union[ConvNextEncoderConfig, ResNetEncoderConfig]


# ============================================================================
# ADAPTER CONFIGS
# ============================================================================

@dataclass
class VerticalFeatureAdapterConfig:
    """
    Configuration for VerticalFeatureAdapter.
    Requires explicit output_dim (should match d_model of sequence model).
    """
    output_dim: Optional[int] = None

@dataclass
class FlattenAdapterConfig:
    """
    Configuration for FlattenAdapter.
    
    User must specify the expected output dimension (C * H * W)
    to ensure the following Head is initialized correctly.
    """
    output_dim: Optional[int] = None

@dataclass
class GlobalPoolingAdapterConfig:
    """Configuration for GlobalPoolingAdapter."""
    pool_type: str = "avg"

@dataclass
class SequencePoolingAdapterConfig:
    """Configuration for SequencePoolingAdapter."""
    pool_type: str = "mean" # mean, max, last, first

AdapterConfig = Union[
    VerticalFeatureAdapterConfig,
    FlattenAdapterConfig,
    GlobalPoolingAdapterConfig,
    SequencePoolingAdapterConfig
]


# ============================================================================
# PROJECTOR CONFIGS
# ============================================================================

@dataclass
class LinearProjectorConfig:
    """Configuration for LinearProjector."""
    pass  # No extra parameters beyond input_dim/output_dim

@dataclass
class IdentityProjectorConfig:
    """Configuration for IdentityProjector."""
    pass  # No extra parameters beyond input_dim/output_dim

@dataclass
class MLPProjectorConfig:
    """Configuration for MLPProjector."""
    
    hidden_dim: Optional[int] = None  # If None, computed as (input_dim + output_dim) // 2
    num_layers: int = 2
    activation: ActivationType = ActivationType.GELU


@dataclass
class BottleneckProjectorConfig:
    """Configuration for BottleneckProjector."""
    
    bottleneck_dim: Optional[int] = None  # If None, computed as min(input_dim, output_dim) // 2
    activation: ActivationType = ActivationType.GELU


@dataclass
class ResidualProjectorConfig:
    """Configuration for ResidualProjector."""
    
    hidden_dim: Optional[int] = None  # If None, computed as (input_dim + output_dim) // 2
    activation: ActivationType = ActivationType.GELU


# Union type for projector configs
ProjectorConfig = Union[
    LinearProjectorConfig,
    MLPProjectorConfig,
    BottleneckProjectorConfig,
    ResidualProjectorConfig,
    IdentityProjectorConfig
]


# ============================================================================
# SEQUENCE MODEL CONFIGS
# ============================================================================

@dataclass
class TransformerConfig(HookedTransformerConfig):
    """
    Base configuration for Transformer-based sequence models.
    Inherits from HookedTransformerConfig to ensure compatibility.
    """
    
    # Override defaults
    n_layers: int = 4
    d_model: int = 256
    n_heads: int = 8
    d_mlp: int = 1024
    n_ctx: int = 384
    d_vocab: int = 62
    d_head: Optional[int] = None
    act_fn: str = "gelu" # Required by HookedTransformerConfig if attn_only=False
    dtype: torch.dtype = torch.float32
    device: Optional[str] = None
    normalization_type: NormalizationType = NormalizationType.RMSNormPre
    
    # Extra fields for our usage
    dropout: float = 0.0
    attn_dropout: float = 0.0
    
    def __post_init__(self):
        # Ensure d_head is computed if not set
        # HookedTransformerConfig's __post_init__ MIGHT handle d_head computation, 
        # but evidently it crashes if n_params calculation happens before that.
        # So we force it here.
        if self.d_head is None and self.d_model is not None and self.n_heads is not None:
             self.d_head = self.d_model // self.n_heads
        
        # Run parent post init
        super().__post_init__()
        
        # Ensure enums are strings if they were passed as Enums (just in case)
        # These fields are now part of HookedTransformerConfig, but we keep the conversion
        # logic in case they are set with Enum types directly.
        if hasattr(self.attention_dir, 'value'): self.attention_dir = self.attention_dir.value
        if hasattr(self.act_fn, 'value'): self.act_fn = self.act_fn.value
        if hasattr(self.positional_embedding_type, 'value'): self.positional_embedding_type = self.positional_embedding_type.value
        if hasattr(self.normalization_type, 'value'): self.normalization_type = self.normalization_type.value


@dataclass
class TransformerEncoderConfig(TransformerConfig):
    """Configuration for TransformerEncoderModel."""
    pass  # Uses base TransformerConfig


@dataclass
class TransformerDecoderConfig(TransformerConfig):
    """Configuration for TransformerDecoderModel (with cross-attention)."""
    pass  # Uses base TransformerConfig, cross-attention is implied


@dataclass
class RNNConfig:
    """Configuration for RNNSequenceModel."""
    
    num_layers: int = 2
    hidden_size: int = 256  # Should match d_model from pipeline
    dropout: float = 0.1
    bidirectional: bool = False
    activation: ActivationType = ActivationType.RELU  # Applied after RNN output


@dataclass
class BiLSTMConfig:
    """Configuration for BiLSTMSequenceModel."""
    
    num_layers: int = 2
    hidden_size: int = 128  # Per direction, total output will be hidden_size * 2
    dropout: float = 0.1
    # Note: bidirectional=True is implicit for BiLSTM


# Union type for sequence model configs
SequenceModelConfig = Union[
    TransformerEncoderConfig,
    TransformerDecoderConfig,
    RNNConfig,
    BiLSTMConfig
]


# ============================================================================
# HEAD CONFIGS
# ============================================================================

@dataclass
class LinearHeadConfig:
    """Configuration for LinearHead."""
    d_model: Optional[int] = None
    d_vocab: Optional[int] = None
    head_type: Optional[str] = None
    loss_type: Optional[str] = None


@dataclass
class MLPHeadConfig:
    """Configuration for MLPHead."""
    
    num_layers: int = 2
    hidden_dim: Optional[int] = None  # If None, computed as d_model * 2
    dropout: float = 0.1
    activation: ActivationType = ActivationType.GELU
    d_model: Optional[int] = None
    d_vocab: Optional[int] = None
    head_type: Optional[str] = None
    loss_type: Optional[str] = None


@dataclass
class ClassificationHeadConfig:
    """Configuration for ClassificationHead."""
    
    num_classes: int = 10
    pooling_type: PoolingType = PoolingType.MEAN
    hidden_dim: Optional[int] = None  # If None, computed as d_model
    activation: ActivationType = ActivationType.GELU
    dropout: float = 0.1
    d_model: Optional[int] = None
    d_vocab: Optional[int] = None
    head_type: Optional[str] = None
    loss_type: Optional[str] = None


# Union type for head configs
HeadConfig = Union[LinearHeadConfig, MLPHeadConfig, ClassificationHeadConfig]


# ============================================================================
# MODEL CONFIG
# ============================================================================

@dataclass
class ModelConfig:
    """Main model configuration consolidating all component configs."""
    
    # Component type selection
    encoder_type: str = 'convnext'
    adapter_type: Optional[str] = None # Existing field, for simple pipelines
    
    # Advanced Pipeline Fields (Encoder -> Adapter -> Sequence -> Adapter -> Head)
    encoder_adapter_type: Optional[str] = None # e.g. 'vertical_feature'
    sequence_adapter_type: Optional[str] = None # e.g. 'sequence_pooling'
    
    projector_type: str = 'linear'
    sequence_model_type: str = 'transformer_encoder'
    head_type: str = 'ctc'
    task_type: TaskType = TaskType.GENERATION
    pipeline_type: Optional[PipelineType] = None
    
    # Component-specific configs
    encoder_config: Optional[EncoderConfig] = None
    adapter_config: Optional[AdapterConfig] = None
    
    encoder_adapter_config: Optional[AdapterConfig] = None
    sequence_adapter_config: Optional[AdapterConfig] = None
    
    projector_config: Optional[ProjectorConfig] = None
    sequence_model_config: Optional[SequenceModelConfig] = None
    head_config: Optional[HeadConfig] = None
    
    # Vocabulary
    d_vocab: int = 62  # Number of characters (excluding blank/pad)
    
    # Loss type
    loss_type: LossType = LossType.CTC
    
    # Pipeline dimensions (auto-resolved if None)
    d_model: Optional[int] = None  # If None, inherited from sequence_model_config or default 256
    
    def __post_init__(self):
        # Infer pipeline type if not set
        if self.pipeline_type is None:
            if self.task_type == TaskType.GENERATION:
                self.pipeline_type = PipelineType.STANDARD_GENERATION
            elif self.task_type == TaskType.CLASSIFICATION:
                self.pipeline_type = PipelineType.STANDARD_CLASSIFICATION
        elif isinstance(self.pipeline_type, str):
            self.pipeline_type = PipelineType(self.pipeline_type)

        # Auto-create configs if not provided, using defaults
        if self.encoder_config is None:
            if self.encoder_type == 'convnext':
                self.encoder_config = ConvNextEncoderConfig()
            elif self.encoder_type == 'resnet':
                self.encoder_config = ResNetEncoderConfig()
            # If a strict registry check is desired, we can add it, 
            # but usually encoders are registered externally.
        
        # Set default adapter type based on task if not provided
        if self.adapter_type is None:
            # Check if we are in a pipeline that allows separate adapters and one is provided
            if (self.pipeline_type == PipelineType.SEQUENCE_CLASSIFICATION or self.encoder_adapter_type is not None):
                 # Allow skipping standard adapter if using advanced pipeline adapters
                 pass 
            else:
                 raise ValueError("Adapter type must be specified for this task type.")
        
        if self.adapter_config is None and self.adapter_type is not None:
            if self.adapter_type == 'vertical_feature':
                self.adapter_config = VerticalFeatureAdapterConfig()
            elif self.adapter_type == 'flatten':
                self.adapter_config = FlattenAdapterConfig()
            elif self.adapter_type == 'global_pool':
                self.adapter_config = GlobalPoolingAdapterConfig()
        
        if self.projector_config is None:
            if self.projector_type == 'linear':
                self.projector_config = LinearProjectorConfig()
            elif self.projector_type == 'mlp':
                self.projector_config = MLPProjectorConfig()
            elif self.projector_type == 'bottleneck':
                self.projector_config = BottleneckProjectorConfig()
            elif self.projector_type == 'residual':
                self.projector_config = ResidualProjectorConfig()
            elif self.projector_type == 'identity':
                self.projector_config = IdentityProjectorConfig() 
            else:
                # Default fallback
                self.projector_config = LinearProjectorConfig()
        
        if self.sequence_model_config is None:
            if self.sequence_model_type == 'transformer_encoder':
                self.sequence_model_config = TransformerEncoderConfig()
            elif self.sequence_model_type == 'transformer_decoder':
                self.sequence_model_config = TransformerDecoderConfig()
            elif self.sequence_model_type == 'rnn':
                self.sequence_model_config = RNNConfig()
            elif self.sequence_model_type == 'bilstm':
                self.sequence_model_config = BiLSTMConfig()
        
        if self.head_config is None:
            if self.head_type == 'linear' or self.head_type == 'ctc':
                self.head_config = LinearHeadConfig()
            elif self.head_type == 'mlp':
                self.head_config = MLPHeadConfig()
            elif self.head_type == 'classification':
                self.head_config = ClassificationHeadConfig()
        
        # Resolve d_model: use from sequence_model_config if available, else default
        if self.d_model is None:
            if isinstance(self.sequence_model_config, (TransformerEncoderConfig, TransformerDecoderConfig)):
                self.d_model = self.sequence_model_config.d_model
            elif isinstance(self.sequence_model_config, RNNConfig):
                self.d_model = self.sequence_model_config.hidden_size
            elif isinstance(self.sequence_model_config, BiLSTMConfig):
                self.d_model = self.sequence_model_config.hidden_size * 2  # Bidirectional
            else:
                raise ValueError("d_model must be specified for this sequence model type.")
        
        # Sync d_model to sequence model config if needed
        if isinstance(self.sequence_model_config, RNNConfig):
            self.sequence_model_config.hidden_size = self.d_model
        elif isinstance(self.sequence_model_config, BiLSTMConfig):
            # BiLSTM hidden_size should be half of d_model
            if self.d_model % 2 != 0:
                raise ValueError(f"d_model ({self.d_model}) must be even for BiLSTM")
            self.sequence_model_config.hidden_size = self.d_model // 2

        self.validate_consistency()

    def validate_consistency(self):
        """Validate consistency of configurations."""
        # 1. Check d_model consistency
        # Only validate sequence config if we are actually using a sequence model
        if self.sequence_model_type and self.sequence_model_config and hasattr(self.sequence_model_config, 'd_model'):
            if self.sequence_model_config.d_model != self.d_model:
                raise ValueError(
                    f"Mismatch in d_model: ModelConfig has {self.d_model}, "
                    f"but SequenceModelConfig has {self.sequence_model_config.d_model}"
                )
        
        if self.head_config and hasattr(self.head_config, 'd_model') and self.head_config.d_model is not None:
             if self.head_config.d_model != self.d_model:
                raise ValueError(
                    f"Mismatch in d_model: ModelConfig has {self.d_model}, "
                    f"but HeadConfig has {self.head_config.d_model}"
                )

        # 2. Check d_vocab consistency
        if self.sequence_model_type and self.sequence_model_config and hasattr(self.sequence_model_config, 'd_vocab'):
             if self.sequence_model_config.d_vocab != self.d_vocab:
                 # Note: TransformerLens config usually requires d_vocab
                raise ValueError(
                    f"Mismatch in d_vocab: ModelConfig has {self.d_vocab}, "
                    f"but SequenceModelConfig has {self.sequence_model_config.d_vocab}"
                )

        if self.head_config and hasattr(self.head_config, 'd_vocab') and self.head_config.d_vocab is not None:
             if self.head_config.d_vocab != self.d_vocab:
                raise ValueError(
                    f"Mismatch in d_vocab: ModelConfig has {self.d_vocab}, "
                    f"but HeadConfig has {self.head_config.d_vocab}"
                )

    
    def get_encoder_output_dim(self) -> int:
        """Get output dimension of the encoder (number of channels)."""
        if self.encoder_config:
            if hasattr(self.encoder_config, 'dims') and self.encoder_config.dims:
                return self.encoder_config.dims[-1]
        # Fallback default if config is missing or has no dims (shouldn't happen with current encoders)
        return 512

    def get_encoder_output_channels(self) -> int:
        """Alias for get_encoder_output_dim."""
        return self.get_encoder_output_dim()

    def get_adapter_output_dim(self) -> int:
        """Get output dimension after adapter."""
        if self.adapter_type == 'vertical_feature':
             if hasattr(self.adapter_config, 'output_dim') and self.adapter_config.output_dim is not None:
                 return self.adapter_config.output_dim
             # Fallback if not specified? 
             # VerticalFeature requires explicit dim.
             # If mapped to old collapse, maybe C?
             # For now return C if undefined, but VerticalFeature logic strongly prefers explicit.
             # But if validation runs before init, user might not have set it?
             # Let's trust explicit or returning C for compatibility if logic matched.
             return self.get_encoder_output_dim()
        elif self.adapter_type == 'global_pool':
             return self.get_encoder_output_dim()
        elif self.adapter_type == 'flatten':
             if hasattr(self.adapter_config, 'output_dim') and self.adapter_config.output_dim is not None:
                 return self.adapter_config.output_dim
             # If explicit dimension is missing, we cannot infer it easily without input shape knowledge.
             # User is expected to provide it as per error message logic in Model.
             return -1 # Should trigger validation error downstream if used
        return self.get_encoder_output_dim()
    
    def get_projector_input_dim(self) -> int:
        """Get projector input dimension (adapter output)."""
        dim = self.get_adapter_output_dim()
        if dim == -1:
             # If dynamic/unknown, we might need to rely on runtime inference or user spec.
             pass
        return dim
    
    def get_projector_output_dim(self) -> int:
        """Get projector output dimension (sequence model input)."""
        return self.d_model
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for logging."""
        return {
            'encoder_type': self.encoder_type,
            'adapter_type': self.adapter_type,
            'projector_type': self.projector_type,
            'sequence_model_type': self.sequence_model_type,
            'head_type': self.head_type,
            'd_vocab': self.d_vocab,
            'd_model': self.d_model,
            'loss_type': self.loss_type.value if isinstance(self.loss_type, LossType) else self.loss_type,
            'task_type': self.task_type.value if isinstance(self.task_type, TaskType) else self.task_type,
            # Sub-configs (as dicts)
            'encoder_config': vars(self.encoder_config) if self.encoder_config else None,
            'adapter_config': vars(self.adapter_config) if self.adapter_config else None,
            'projector_config': vars(self.projector_config) if self.projector_config else None,
            'sequence_model_config': self.sequence_model_config.to_dict() if hasattr(self.sequence_model_config, 'to_dict') else (vars(self.sequence_model_config) if self.sequence_model_config else None),
            'head_config': vars(self.head_config) if self.head_config else None,
        }


# ============================================================================
# TRAINING CONFIG
# ============================================================================

@dataclass
class TrainingConfig:
    """Configuration for training hyperparameters."""
    
    # Basic training parameters
    batch_size: int = 32
    epochs: int = 50
    learning_rate: float = 1e-4
    
    # Step-based training
    training_steps: Optional[int] = None # If set, overrides epochs logic for loop duration
    use_onthefly_generation: bool = False
    save_every_steps: int = 2048
    val_check_interval_steps: int = 2048
    val_steps: int = 50 # Number of batches to validate per check
    
    # Optimizer configuration
    optimizer_type: OptimizerType = OptimizerType.ADAMW
    weight_decay: float = 0.01
    beta1: float = 0.9
    beta2: float = 0.999
    eps: float = 1e-8
    
    # Learning rate schedule
    lr_scheduler_type: Optional[str] = None  # e.g., "cosine", "step", "plateau"
    lr_scheduler_params: Dict[str, Any] = field(default_factory=dict)
    lr_scheduler_step_unit: str = "step" # "epoch" or "step" (batch)
    
    # Gradient configuration
    grad_clip_norm: float = 1.0
    accumulate_grad_batches: int = 1
    
    # Device and parallelism
    device: str = "cuda"  # "cuda" or "cpu"
    num_workers: int = 4
    mixed_precision: bool = False
    
    # Checkpointing
    checkpoint_dir: str = "checkpoints"
    save_every_n_epochs: int = 1
    save_best_only: bool = False
    monitor_metric: str = "val_exact_match"  # Metric to monitor for best model
    
    # Logging
    wandb_project: str = "captcha-ocr"
    wandb_run_name: Optional[str] = None
    log_every_n_steps: int = 10
    
    # Validation
    val_split: float = 0.1  # Fraction of dataset to use for validation
    val_check_interval: float = 1.0  # Validate every N epochs
    
    # Data loading
    shuffle_train: bool = True
    pin_memory: bool = True
    
    # Metrics
    metrics: Optional[List[str]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for logging."""
        return {
            'batch_size': self.batch_size,
            'epochs': self.epochs,
            'learning_rate': self.learning_rate,
            'optimizer_type': self.optimizer_type.value if isinstance(self.optimizer_type, OptimizerType) else self.optimizer_type,
            'weight_decay': self.weight_decay,
            'grad_clip_norm': self.grad_clip_norm,
            'device': self.device,
            'training_steps': self.training_steps,
            'use_onthefly_generation': self.use_onthefly_generation,
            'save_every_steps': self.save_every_steps,
            'val_check_interval_steps': self.val_check_interval_steps,
            'lr_scheduler_type': self.lr_scheduler_type,
            'lr_scheduler_params': self.lr_scheduler_params,
            'lr_scheduler_step_unit': self.lr_scheduler_step_unit,
        }


# ============================================================================
# EXPERIMENT CONFIG (MAIN CONFIG)
# ============================================================================

@dataclass
class ExperimentConfig:
    """
    Main experiment configuration consolidating all sub-configs.
    
    This is the single point of truth for all hyperparameters.
    """
    
    # Experiment metadata
    experiment_name: str = "default_experiment"
    seed: Optional[int] = None
    
    # Sub-configs
    dataset_config: DatasetConfig = field(default_factory=DatasetConfig)
    model_config: ModelConfig = field(default_factory=ModelConfig)
    training_config: TrainingConfig = field(default_factory=TrainingConfig)
    
    # Paths
    metadata_path: str = "validation_set/metadata.json"
    train_metadata_path: Optional[str] = None
    val_metadata_path: Optional[str] = None
    image_base_dir: str = "."
    
    def __post_init__(self):
        """Validate and sync configurations."""
        # Sync dataset height with model input height expectation
        # This ensures consistency between data generation and model input
        
        pass
        
        # Sync RNN/LSTM hidden sizes with d_model if needed
        if isinstance(self.model_config.sequence_model_config, RNNConfig):
            self.model_config.sequence_model_config.hidden_size = self.model_config.d_model
        elif isinstance(self.model_config.sequence_model_config, BiLSTMConfig):
            if self.model_config.d_model % 2 != 0:
                raise ValueError("d_model must be even for BiLSTM")
            self.model_config.sequence_model_config.hidden_size = self.model_config.d_model // 2
        

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for logging/serialization."""
        return {
            'experiment_name': self.experiment_name,
            'seed': self.seed,
            'dataset': self.dataset_config.to_dict(),
            'model': self.model_config.to_dict(),
            'training': self.training_config.to_dict(),
        }
    
    def describe(self) -> str:
        """Generate human-readable description."""
        return f"""
Experiment: {self.experiment_name}
{'=' * 60}

Dataset:
  Height: {self.dataset_config.height}
  Width divisor: {self.dataset_config.width_divisor}
  Width bias: {self.dataset_config.width_bias}

Model:
  Encoder: {self.model_config.encoder_type} (out_dim={self.model_config.get_encoder_output_dim()})
  Projector: {self.model_config.projector_type}
  Sequence: {self.model_config.sequence_model_type}
  Head: {self.model_config.head_type}
  d_model: {self.model_config.d_model}
  d_vocab: {self.model_config.d_vocab}

Training:
  Batch size: {self.training_config.batch_size}
  Epochs: {self.training_config.epochs}
  Learning rate: {self.training_config.learning_rate}
  Optimizer: {self.training_config.optimizer_type}
{'=' * 60}
        """