experiment_name: "classification_baseline"
seed: 42
train_metadata_path: "dataset_train/dataset/metadata.json"
val_metadata_path: "dataset_val/dataset/metadata.json"

dataset_config:
  width: 200
  height: 80
  width_divisor: 4
  width_bias: 0

model_config:
  task_type: "classification"
  encoder_type: "asymmetric_convnext"
  encoder_config:
    dims: [32, 64, 128, 256]
    stage_block_counts: [1, 1, 3, 1]
  
  projector_type: "linear"
  
  # Optional: Use transformer to mix features before pooling
  sequence_model_type: "rnn"
  sequence_model_config:
    d_model: 64
    d_vocab: 100 # Must match model d_vocab
    n_layers: 2
    
  head_type: "classification"
  head_config:
    num_classes: 100 # Example, override in code or config if known
    pooling_type: "mean"
    d_model: 64
  
  d_model: 64
  d_vocab: 100 # Same as num_classes roughly
  loss_type: "cross_entropy"

training_config:
  batch_size: 32
  epochs: 10
  learning_rate: 0.0001
  optimizer_type: "adamw"
  checkpoint_dir: "experiments/classification/aconvnext_rnn_1/checkpoints"
