experiment_name: "resnet_base"
# Goal: Get a baseline for classification

seed: 42
train_metadata_path: "experiments/dataset/clean/train/metadata.json"
val_metadata_path: "experiments/dataset/clean/val/metadata.json"
# image_base_dir: "dataset_train/dataset" # necessary only if train_metadata_path and val_metadata_path are not provided

# A clean dataset, with 100 words, and 5 letters per captcha
# Capitalization is mixed, and fonts are randomly chosen, but no other noise
# 20 fonts are taken in total

# At max, there are (20)^5 variations for a single captcha
# This gives a maximum dataset size of 100 * (20^5) = 3.2 * 10^8

# However, the number of unique characters are:
# 26 (lowercase) + 26 (uppercase) + 10 (digits) = 62
# Number of unique fonts: 20
# Unique glyphs: 62 * 20 = 1240

dataset_config:
  # Dimensions to generate/resize images to
  width: 192
  height: 64
  width_divisor: 4 # Requirement when using variable width
  width_bias: 0
  resize_mode: "fixed"
  image_ext: "png"
  
  # Font & Noise settings (used for generation) For training, these are saved in config, but are not useful for the model
  max_fonts_per_family: 1 
  random_capitalize: False
  noise_bg_density: 0
  add_noise_dots: False
  add_noise_curve: False
  extra_spacing: 0
  spacing_jitter: 0
  word_space_probability: 0
  word_offset_dx: 0.0

# Model Config
# This set up results in about 500k params
# The stem is formed by 4*4 kernels with stride 4
# Most characters are wider than 4 px across.

model_config:
  pipeline_type: "standard_classification" # "standard_classification" or "sequence_classification"
  task_type: "classification" # "classification" or "generation"
  
  # --- 1. Encoder (Backbone) ---
  encoder_type: "resnet" # Recommended for classification
  encoder_config:
    # ResNet dimensions per stage
    dims: [8, 16, 24, 48] 
    stage_block_counts: [2, 2, 6, 2]
    strides: [(2, 2), (2, 2), (2, 2), (2, 2)] 
    # ([B, C, 64, 192] -> [B, C, 16, 48]) -> ([B, C, 8, 24] -> [B, C, 4, 12] -> [B, C, 2, 6])
    
  # --- 2. Adapter (Spatial -> Vector) ---
  adapter_type: "flatten" # "flatten", "global_pool", or "vertical_feature"
  adapter_config:
    # IMPORTANT: Valid ONLY for 80x200 input and ResNet encoder.
    # ResNet output is [Batch, 48, H/32, W/32].
    # H=80 -> 2. W=200 -> 6. Result: 48 * 2 * 6 = 576.
    output_dim: 576  
  
  # --- 3. Projector (Optional) ---
  projector_type: null # Not needed if Head can take Adapter output directly
  
  # --- 4. Sequence Model (Not used for Classification) ---
  sequence_model_type: null
    
  # --- 5. Head (Vector -> Class Logits) ---
  head_type: "classification"
  head_config:
    num_classes: 100 # Example: 10 digit classes
    d_model: 576 # Must match Adapter output dimension
    head_hidden_dim: 256 # Intermediate dense layer size
    pooling_type: "mean" # Ignored if input is already flat/2D, but good to keep
  
  # Global Params
  d_model: 576 # Matches pipeline width at head input
  d_vocab: 100 # Roughly matches num_classes
  loss_type: "cross_entropy"

training_config:
  batch_size: 32
  epochs: 50
  learning_rate: 0.0001
  optimizer_type: "adamw" # adam, adamw, sgd
  weight_decay: 0.01
  grad_clip_norm: 1.0
  accumulate_grad_batches: 1
  
  # Checkpointing & Logging
  checkpoint_dir: "experiments/classification/checkpoints/resnet_base"
  save_every_n_epochs: 1
  monitor_metric: "val_acc" # Monitor accuracy for classification
  wandb_project: "captcha-ocr"
  log_every_n_steps: 10
  
  # Hardware & Data
  device: "cuda"
  num_workers: 4
  mixed_precision: false
  #val_split: 0.1 # necessary only if train_metadata_path and val_metadata_path are not provided
  shuffle_train: true
  metrics: ['accuracy', 'f1', 'precision', 'recall']
