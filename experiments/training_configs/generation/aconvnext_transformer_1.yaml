experiment_name: "generation_baseline"
seed: 42
train_metadata_path: "dataset_train/dataset/metadata.json"
val_metadata_path: "dataset_val/dataset/metadata.json"
#image_base_dir: "dataset_train/dataset"

dataset_config:
  # Dimensions
  width: 200
  height: 80
  width_divisor: 16
  width_bias: 0
  image_ext: "png"
  resize_mode: "variable"
  
  # Generation Params
  max_fonts_per_family: 1
  random_capitalize: False
  noise_bg_density: 0
  add_noise_dots: False
  add_noise_curve: False
  extra_spacing: 0
  spacing_jitter: 0


model_config:
  task_type: "generation"
  pipeline_type: "standard_generation" # only available pipeline for generation
  
  # --- 1. Encoder (Backbone) ---
  encoder_type: "convnext"
  encoder_config:
    dims: [16, 32, 64, 128]
    stage_block_counts: [1, 1, 3, 1]
    
  # --- 2. Adapter (Spatial -> Sequence) ---
  adapter_type: "vertical_feature"
  adapter_config:
     output_dim: 512 
  
  # --- 3. Projector (Encoder Dim -> Model Dim) ---
  projector_type: "linear" # Projects 512 -> 256
  
  # --- 4. Sequence Model (Seq -> Seq) ---
  sequence_model_type: "transformer_encoder"
  sequence_model_config:
    d_model: 64
    n_layers: 6
    n_heads: 8
    d_mlp: 128
    dropout: 0.1
    max_len: 200 # Max expected sequence length
    
  # --- 5. Head (Seq -> Logits) ---
  head_type: "ctc"
  head_config:
    d_model: 64
    d_vocab: 62 # Alphanumeric char set
  
  # Global Params
  d_model: 64
  d_vocab: 62
  loss_type: "ctc"

training_config:
  batch_size: 32
  epochs: 50
  learning_rate: 0.0001
  optimizer_type: "adamw"
  weight_decay: 0.01
  grad_clip_norm: 1.0
  accumulate_grad_batches: 1
  
  checkpoint_dir: "experiments/generation/aconvnext_transformer_1/checkpoints"
  save_every_n_epochs: 1
  monitor_metric: "val_exact_match"
  wandb_project: "captcha-ocr"
  log_every_n_steps: 10
  
  device: "cuda"
  num_workers: 4
  mixed_precision: false
  val_split: 0.1
  shuffle_train: true
  metrics: ['character_accuracy', 'word_correct', 'edit_distance']
