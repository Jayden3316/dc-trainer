{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "5132bd54",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\jerry\\IITM\\CFI\\Ai Club\\2025-26\\Precog\\captcha_ocr\\notebooks\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "print(os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47b3b9fd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate 100 random words\n",
                "\n",
                "import random\n",
                "import string\n",
                "\n",
                "def random_capitalize(word):\n",
                "    return ''.join(random.choice([c.upper(), c.lower()]) for c in word)\n",
                "\n",
                "vocab = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
                "\n",
                "words = []\n",
                "for _ in range(100):\n",
                "    word = ''.join(random.choice(vocab) for _ in range(5))\n",
                "    words.append(word)\n",
                "\n",
                "# Use columns: ['word_id', 'word', 'frequency']\n",
                "# frequency is dummy, just for compatibility with wiki_words used in generate_captchas\n",
                "\n",
                "word_id = [idx for idx in range(len(words))]\n",
                "\n",
                "\n",
                "with open('../diverse_words.tsv', 'w') as f:\n",
                "    for word_id, word in enumerate(words):\n",
                "        f.write(f\"{word_id}\\t{word}\\t1\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "298b2e3b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Dataset Verification ---\n",
                        "Config: experiments/training_configs/classification/aconvnext_rnn_1.yaml\n",
                        "Train set: ..\\dataset_train\\dataset\\metadata.json (2000 samples)\n",
                        "Val set:   ..\\dataset_test\\metadata.json (400 samples)\n",
                        "\n",
                        "Classes in train set: 100\n",
                        "Classes in val set:   99\n",
                        "WARNING: 1 classes in train but NOT in val.\n",
                        "Note: This is expected if 'generate' used random sampling with replacement (dataset_count > vocab_size).\n",
                        "\n",
                        "--- Processor Verification ---\n",
                        "Processor class count: 100\n",
                        "SUCCESS: Processor vocabulary matches metadata.\n",
                        "Sample classes: ['0s637', '17EmL', '2H5au', '2NrY4', '2tNwn']...\n"
                    ]
                }
            ],
            "source": [
                "# verifying the train and validation datasets\n",
                "\n",
                "import sys\n",
                "import yaml\n",
                "import json\n",
                "from pathlib import Path\n",
                "from typing import List, Dict\n",
                "\n",
                "# Add project root to path so we can import src\n",
                "if \"../\" not in sys.path:\n",
                "    sys.path.append(\"../\")\n",
                "\n",
                "from src.processor import CaptchaProcessor, ClassificationProcessor\n",
                "from src.config.loader import hydrate_config\n",
                "from src.config.config import ExperimentConfig\n",
                "\n",
                "def load_config(config_path: str) -> ExperimentConfig:\n",
                "    \"\"\"Load config from YAML.\"\"\"\n",
                "    path = Path(config_path)\n",
                "    if not path.exists():\n",
                "        # Try relative to project root\n",
                "        path = Path(\"../\") / config_path\n",
                "        if not path.exists():\n",
                "            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
                "    \n",
                "    with open(path, 'r') as f:\n",
                "        data = yaml.safe_load(f)\n",
                "        \n",
                "    return hydrate_config(data)\n",
                "\n",
                "# Configuration parameters\n",
                "config_path = \"experiments/training_configs/classification/aconvnext_rnn_1.yaml\"\n",
                "config = load_config(config_path)\n",
                "\n",
                "# Metadata paths from config (if available) or defaults\n",
                "train_set_path =  \"dataset_train/dataset/metadata.json\"\n",
                "val_set_path = \"dataset_test/metadata.json\"\n",
                "\n",
                "# Helper to resolve path relative to project root if running in notebook\n",
                "def resolve_path(p):\n",
                "    path = Path(p)\n",
                "    if not path.exists():\n",
                "        # Try one level up\n",
                "        alt = Path(\"../\") / p\n",
                "        if alt.exists():\n",
                "            return str(alt)\n",
                "    return str(path)\n",
                "\n",
                "train_set_path = resolve_path(train_set_path)\n",
                "val_set_path = resolve_path(val_set_path)\n",
                "\n",
                "print(f\"--- Dataset Verification ---\")\n",
                "print(f\"Config: {config_path}\")\n",
                "\n",
                "try:\n",
                "    # load_metadata now just needs to open the resolved path\n",
                "    with open(train_set_path, 'r') as f:\n",
                "        train_set = json.load(f)\n",
                "    with open(val_set_path, 'r') as f:\n",
                "        val_set = json.load(f)\n",
                "    \n",
                "    print(f\"Train set: {train_set_path} ({len(train_set)} samples)\")\n",
                "    print(f\"Val set:   {val_set_path} ({len(val_set)} samples)\")\n",
                "    \n",
                "    # Finding classes from metadata\n",
                "    # Using 'word_input' as per recent processor update\n",
                "    train_classes = sorted(list(set(entry.get('word_input', entry.get('word_rendered', '')) for entry in train_set)))\n",
                "    val_classes = sorted(list(set(entry.get('word_input', entry.get('word_rendered', '')) for entry in val_set)))\n",
                "\n",
                "    print(f\"\\nClasses in train set: {len(train_classes)}\")\n",
                "    print(f\"Classes in val set:   {len(val_classes)}\")\n",
                "    \n",
                "    # Consistency Check\n",
                "    missing_in_val = set(train_classes) - set(val_classes)\n",
                "    if missing_in_val:\n",
                "        print(f\"WARNING: {len(missing_in_val)} classes in train but NOT in val.\")\n",
                "        print(\"Note: This is expected if 'generate' used random sampling with replacement (dataset_count > vocab_size).\")\n",
                "    else:\n",
                "        print(\"SUCCESS: Validation set covers all training classes.\")\n",
                "\n",
                "    # Verifying with Processor\n",
                "    print(f\"\\n--- Processor Verification ---\")\n",
                "    # This will use the metadata_path to build the internal vocab/classes list\n",
                "    # Use resolved train_set_path\n",
                "    config.metadata_path = train_set_path\n",
                "    processor = ClassificationProcessor(config, metadata_path=train_set_path)\n",
                "    \n",
                "    print(f\"Processor class count: {len(processor.classes)}\")\n",
                "    if len(processor.classes) == len(train_classes):\n",
                "        print(\"SUCCESS: Processor vocabulary matches metadata.\")\n",
                "    else:\n",
                "        print(f\"FAILURE: Mismatch! Processor has {len(processor.classes)} vs Metadata {len(train_classes)}\")\n",
                "        \n",
                "    # Optional: print first 5 classes\n",
                "    print(f\"Sample classes: {processor.classes[:5]}...\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"ERROR during verification: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "ca57ee41",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\jerry\\IITM\\CFI\\Ai Club\\2025-26\\Precog\\captcha_ocr\\notebooks\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "print(os.getcwd())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "c64cce87",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dict_keys(['epoch', 'state_dict', 'optimizer_state_dict', 'config', 'metrics'])\n"
                    ]
                }
            ],
            "source": [
                "# loading and verifying checkpoint\n",
                "import torch \n",
                "\n",
                "checkpoint_path = r\"..\\experiments\\classification\\aconvnext_rnn_1\\checkpoints\\best_classification_baseline.pth\"\n",
                "\n",
                "model = torch.load(checkpoint_path)\n",
                "print(model.keys())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "c5c3b94c",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'experiment_name': 'classification_baseline',\n",
                            " 'seed': 42,\n",
                            " 'dataset': {'width': 200,\n",
                            "  'height': 80,\n",
                            "  'noise_bg_density': 5000,\n",
                            "  'add_noise_dots': True,\n",
                            "  'add_noise_curve': True,\n",
                            "  'extra_spacing': -5,\n",
                            "  'spacing_jitter': 6,\n",
                            "  'target_height': 80,\n",
                            "  'width_divisor': 4,\n",
                            "  'width_bias': 0},\n",
                            " 'model': {'encoder_type': 'asymmetric_convnext',\n",
                            "  'projector_type': 'linear',\n",
                            "  'sequence_model_type': 'rnn',\n",
                            "  'head_type': 'classification',\n",
                            "  'd_vocab': 100,\n",
                            "  'd_model': 64,\n",
                            "  'loss_type': 'cross_entropy',\n",
                            "  'task_type': 'classification'},\n",
                            " 'training': {'batch_size': 32,\n",
                            "  'epochs': 10,\n",
                            "  'learning_rate': 0.0001,\n",
                            "  'optimizer_type': 'adamw',\n",
                            "  'weight_decay': 0.01,\n",
                            "  'grad_clip_norm': 1.0,\n",
                            "  'device': 'cuda'}}"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model['config']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "673e83a8",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "precog",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
